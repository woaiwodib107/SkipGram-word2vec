{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17075036.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHANMUKHA-VENKAT/SkipGram-word2vec/blob/master/17075036.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WuDQD_69Cx8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Artificial Intelligence Lab 2019: Word2Vec Assignment**\n",
        "\n",
        "Hello Junta! In this Assignment you will be training your own word vectors. To this end, you will be using Word2Vec technique proposed by Tomas Mikolov (and others) at Google. \n",
        "Pre-requisites:\n",
        " - Word2Vec Basics\n",
        " - Neural Networks\n",
        " - Know-how aboutTensorflow, Numpy and MatplotLib\n",
        "\n",
        "Well there are two methods for word2vec: Skip-Gram and CBOW. In this assignment, you will be working on SkipGram.\n",
        "\n",
        "I expect you integrity on your side while doing this assignment.  Ans please do not modify the code (for submission) unless explicitely stated.\n",
        "\n",
        "Enter Your \n",
        "- Name : Narayana Shanmukha Venkat\n",
        "- Roll : 17075036\n",
        "\n",
        "\n",
        "All the Best!"
      ]
    },
    {
      "metadata": {
        "id": "OM_1AW8DDSf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# import all the necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "RVBw48hkDWAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "# tqdm is used in this code for progress bar which you will see while training.\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(12345)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(12345)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLZMO6iB1YIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DataReader"
      ]
    },
    {
      "metadata": {
        "id": "FMEV8S640t8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataReader:\n",
        "    \"\"\"The data you will be using for this assignment is PennTreebank Data (which has been provided saperately).\"\"\"\n",
        "    NEGATIVE_TABLE_SIZE = 1e8\n",
        "\n",
        "    def __init__(self, inputFileName, min_count):\n",
        "\n",
        "        self.negatives = []\n",
        "        self.discards = []\n",
        "        self.negpos = 0\n",
        "\n",
        "        self.word2id = dict() # Maps words to indexes\n",
        "        self.id2word = dict() # Maps indexes to words\n",
        "        self.sentences_count = 0 # stores the number of sentences in the corpus\n",
        "        self.token_count = 0 # stores the number of tokens in the corpus\n",
        "        self.word_frequency = dict() # Maps words indices to how many times they occur in the corpus. This dictionary will be helpful later when you update 'word2id' and 'id2word'\n",
        "        # But NOTE <self.word_frequency> is a mapping from INDICES, not words.\n",
        "\n",
        "        self.inputFileName = inputFileName\n",
        "        self.read_words(min_count) \n",
        "        self.initTableNegatives()\n",
        "        self.initTableDiscards()\n",
        "\n",
        "    def read_words(self, min_count):\n",
        "        \"\"\"Do not return anything from this function. Motive here is to update the dictionaries word2id and id2word.\n",
        "             - first update the word_frequency dictionary.\n",
        "        \"\"\"\n",
        "        word_frequency = dict()\n",
        "        for line in open(self.inputFileName, encoding=\"utf8\"):\n",
        "            ### YOUR CODE STARTS HERE\n",
        "            # lowercase the text. split it on white_spaces\n",
        "            # keep updating the sentences_count, token_count, word_frequency as you go\n",
        "            \n",
        "            line = line.lower()\n",
        "            self.sentences_count += 1\n",
        "            for w in line.split():\n",
        "                if len(w) > 0:\n",
        "                    self.token_count += 1\n",
        "                    word_frequency[w] = word_frequency.get(w, 0) + 1\n",
        "\n",
        "                    if self.token_count % 1000000 == 0:\n",
        "                        print(\"Read \" + str(int(self.token_count / 1000000)) + \"M words.\")\n",
        "\n",
        "            \n",
        "            ### YOUR CODE ENDS HERE\n",
        "            # Uncomment these lines and feel free to bring them in your loop of data_reading, to keep track of how much data is read\n",
        "            # if self.token_count % 1000000 == 0:\n",
        "            #     print(\"Read \" + str(int(self.token_count / 100000)) + \"M words.\")\n",
        "\n",
        "        wid = 0\n",
        "        for w, c in word_frequency.items():\n",
        "            ### YOUR CODE STARTS HERE\n",
        "            # do nothing if the word_frequency (c) is less than min_count\n",
        "            # update the word2id and id2word using w and wid.\n",
        "            # accordingly update the self.word_frequency as you go; Note that <wid> goes in the keys of this dictionary.\n",
        "            if c < min_count:\n",
        "                continue\n",
        "            self.id2word[wid] = w\n",
        "            self.word2id[w] = wid\n",
        "            self.word_frequency[wid] = c\n",
        "            wid += 1\n",
        "            \n",
        "            ### YOUR CODE ENDS HERE\n",
        "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
        "\n",
        "    def initTableDiscards(self):\n",
        "        t = 0.0001\n",
        "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
        "        self.discards = np.sqrt(t / f) + (t / f)\n",
        "\n",
        "    def initTableNegatives(self):\n",
        "        # Remember the negative sampling part. \n",
        "        # You already calculated the self.word_frequency\n",
        "        # Now raise all these frequencies to a power of 0.5\n",
        "        # normalize the them by their sum: store it in ratio.\n",
        "        ratio = None\n",
        "        \n",
        "        ### YOUR CODE STARTS HERE\n",
        "        powered_frequency_list = np.array(list(self.word_frequency.values())) ** 0.5\n",
        "        ratio = powered_frequency_list / sum(powered_frequency_list)\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        \n",
        "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
        "        \n",
        "        # We prepare the negatives list using the above parameters for negative sampling.\n",
        "        for wid, c in enumerate(count):\n",
        "            self.negatives += [wid] * int(c)\n",
        "        \n",
        "        ### YOUR CODE STARTS HERE\n",
        "        self.negatives = np.array(self.negatives)\n",
        "        np.random.shuffle(self.negatives)\n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "    def getNegatives(self, target, size):\n",
        "        response = self.negatives[self.negpos:self.negpos + size]\n",
        "        self.negpos = (self.negpos + size) % len(self.negatives)\n",
        "        if len(response) != size:\n",
        "            return np.concatenate((response, self.negatives[0:self.negpos]))\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zzRXUwM7MOV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Word2vecDataset(Dataset):\n",
        "    def __init__(self, data, window_size):\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.input_file = open(data.inputFileName, encoding=\"utf8\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.sentences_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            line = self.input_file.readline()\n",
        "            if not line:\n",
        "                self.input_file.seek(0, 0)\n",
        "                line = self.input_file.readline()\n",
        "\n",
        "            if len(line) > 1:\n",
        "                words = line.split()\n",
        "\n",
        "                if len(words) > 1:\n",
        "                    word_ids = [self.data.word2id[w] for w in words if\n",
        "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
        "\n",
        "                    boundary = np.random.randint(1, self.window_size)\n",
        "                    return [(u, v, self.data.getNegatives(v, 5)) for i, u in enumerate(word_ids) for j, v in\n",
        "                            enumerate(word_ids[max(i - boundary, 0):i + boundary]) if u != v]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
        "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
        "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
        "\n",
        "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1p5dcf1P1ynE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Word2VecTrainer:\n",
        "    def __init__(self, input_file, output_file, emb_dimension=100, batch_size=32, window_size=5, iterations=10,\n",
        "                 initial_lr=0.001, min_count=5):\n",
        "        \"\"\"Please do not change emb_dimension, min_count, output_file. Other wise you might get penalized (automatically by the checker). Feel free to change other params.\"\"\"\n",
        "        self.data = DataReader(input_file, min_count)\n",
        "        dataset = Word2vecDataset(self.data, window_size)\n",
        "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                     shuffle=False, num_workers=0, collate_fn=dataset.collate)\n",
        "\n",
        "        self.output_file_name = output_file\n",
        "        self.emb_size = len(self.data.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.iterations = iterations\n",
        "        self.initial_lr = initial_lr\n",
        "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "        if self.use_cuda:\n",
        "            self.skip_gram_model.cuda()\n",
        "\n",
        "    def train(self):\n",
        "        for iteration in range(self.iterations):\n",
        "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
        "            optimizer = optim.SparseAdam(self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
        "                if len(sample_batched[0]) > 1:\n",
        "                    pos_u = sample_batched[0].to(self.device)\n",
        "                    pos_v = sample_batched[1].to(self.device)\n",
        "                    neg_v = sample_batched[2].to(self.device)\n",
        "                    \n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
        "                    if i > 0 and i % 500 == 0:\n",
        "                        print(\" Loss: \" + str(running_loss))\n",
        "\n",
        "        self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)\n",
        "        return self.skip_gram_model.embedding_dictionary(self.data.id2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPjWgzLG1XmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"This is a pytorch implementation for SkipGram Model. \n",
        "       Note here we use two different embeddings \n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neighbor words.\n",
        "       And finally we save the u_embedding as the word vectors. But Note that you are free to experiment with using the v_embeddings/ or averaging them both.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Here you define the layers of the model.\"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings, self.v_embeddings = None, None\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        # initialize the u_embeddings, v_embeddings with Embedding layer from Pytorch. \n",
        "        # refer https://pytorch.org/docs/stable/nn.html#sparse-layers\n",
        "        # *** NOTE: keep the sparse parameter of the layer True\n",
        "        # set the dimension of them both to (emb_size, emb_dimension)\n",
        "\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        \n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "        initrange = 1.0 / self.emb_dimension\n",
        "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
        "        init.constant_(self.v_embeddings.weight.data, 0)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"Here you define how the layers process the input of the neural network.\n",
        "        Return the sum of <score> and <negative score>\n",
        "        \"\"\"\n",
        "        # the tensors pos_u, pos_v, neg_v contain the indices of the words\n",
        "        # now you need to map these indices to their word_embeddings\n",
        "        # simply pass these above tensors through the u_embeddings and v_embeddings layers and store the result in the following variables. Do not reshape them.\n",
        "        # And that's it. Pretty easy isn't  (^_^')\n",
        "        \n",
        "        emb_u, emb_v, emb_neg_v = None, None, None\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        \n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        emb_neg_v = self.v_embeddings(neg_v)\n",
        "        \n",
        "        ### YOUR CODE ENDS HERE  \n",
        "        \n",
        "        # now take the dot product of the emb_u and emb_v, clamp the score and normalize it through softmax.\n",
        "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
        "        score = torch.clamp(score, max=10, min=-10)\n",
        "        score = -F.logsigmoid(score)\n",
        "        \n",
        "        #  the following piece of code calculated the negative sampling score\n",
        "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
        "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
        "\n",
        "        return torch.mean(score + neg_score)\n",
        "\n",
        "    def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "            for wid, w in id2word.items():\n",
        "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
        "                f.write('%s %s\\n' % (w, e))\n",
        "    \n",
        "    def embedding_dictionary(self, id2word):\n",
        "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        E = {}\n",
        "        for wid, w in id2word.items():\n",
        "            E[w] = embedding[wid]\n",
        "        return E"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LocBN4Lo3KTx",
        "colab_type": "code",
        "outputId": "c3833b25-fc68-420c-8ef0-bd9e38f85f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive Here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Upload the data files on the Drive\n",
        "# Set the path to the \"polarity.pkl\" file\n",
        "data_path = \"gdrive/My Drive/polarity.pkl\"\n",
        "\n",
        "\n",
        "# check if the path you want to use for  exists\n",
        "!cd \"gdrive/My Drive/word2vec\" & echo \"Hello\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Hello\n",
            "/bin/bash: line 0: cd: gdrive/My Drive/word2vec: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XOY2FUKG2uVm",
        "colab_type": "code",
        "outputId": "48962f0c-7d75-4b66-be40-9b03da3b1bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "input_file = 'gdrive/My Drive/PTBclean.txt'\n",
        "w2v = Word2VecTrainer(input_file=input_file, output_file=\"out.vec\")\n",
        "vectors = w2v.train()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total embeddings: 4487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 7/739 [00:00<00:11, 66.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 510/739 [00:07<00:03, 73.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.707456116543185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 72.24it/s]\n",
            "  1%|          | 7/739 [00:00<00:11, 62.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 515/739 [00:07<00:03, 72.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.2406104540521494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 69.45it/s]\n",
            "  1%|          | 7/739 [00:00<00:11, 65.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 515/739 [00:07<00:03, 73.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.9928037703632886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 73.29it/s]\n",
            "  1%|          | 7/739 [00:00<00:12, 59.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 509/739 [00:06<00:03, 74.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8868987894962816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:09<00:00, 75.15it/s]\n",
            "  1%|          | 8/739 [00:00<00:09, 74.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 509/739 [00:07<00:03, 72.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8267869576960845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 73.26it/s]\n",
            "  1%|          | 7/739 [00:00<00:10, 68.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 509/739 [00:06<00:03, 73.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.766704698791863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 73.62it/s]\n",
            "  1%|          | 8/739 [00:00<00:10, 72.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 512/739 [00:06<00:03, 75.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.7358079956575967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:09<00:00, 74.54it/s]\n",
            "  1%|          | 8/739 [00:00<00:10, 71.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 515/739 [00:06<00:03, 73.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.699772361863685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:10<00:00, 73.74it/s]\n",
            "  1%|          | 8/739 [00:00<00:10, 72.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 515/739 [00:06<00:02, 75.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.6604878783248265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:09<00:00, 74.26it/s]\n",
            "  1%|          | 8/739 [00:00<00:09, 73.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 520/739 [00:06<00:02, 84.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.68866715342172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [00:09<00:00, 74.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NZ3QQgd1gUNx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualize your vectors"
      ]
    },
    {
      "metadata": {
        "id": "hLivWrly5DHt",
        "colab_type": "code",
        "outputId": "febf386d-079d-490d-f970-122ef66af9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "print(vectors['is'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.08327655 -0.06465846  0.07731729 -0.02627495 -0.05345292 -0.06204605\n",
            " -0.05429661  0.06235377 -0.05870133  0.0950086   0.03880911  0.0715967\n",
            "  0.05601985  0.09041788 -0.06442855 -0.04428674 -0.06773079 -0.01808022\n",
            "  0.02482437 -0.11535365 -0.08372795  0.08389358  0.05334435  0.06869959\n",
            " -0.03140078  0.0710893  -0.03604905  0.07418153 -0.07196469 -0.06176475\n",
            " -0.05916577  0.05566907  0.02862125 -0.04528474  0.01226763 -0.01991124\n",
            " -0.06892862 -0.04671647 -0.08352187 -0.05258016  0.04866216  0.08052907\n",
            " -0.04682224 -0.06365536 -0.0681692  -0.03767868  0.05861996  0.0539942\n",
            "  0.06059917 -0.08365571 -0.07653565 -0.09868433  0.09856103  0.07701399\n",
            " -0.02827692  0.09128118  0.05811524 -0.04007667  0.00365094 -0.0299638\n",
            "  0.09596469  0.06692669 -0.06587201  0.09267444  0.06228261  0.04927918\n",
            "  0.0439609  -0.06128385 -0.07278035  0.12482908 -0.08201605  0.06444329\n",
            " -0.12446178 -0.05977924 -0.05795981 -0.08257019 -0.03675573  0.07518455\n",
            "  0.09716192  0.06552763  0.06371452  0.06031176  0.04118165  0.01858202\n",
            "  0.07347772  0.07043028 -0.07102254 -0.09160911  0.07739732 -0.0851656\n",
            " -0.04617786  0.03903497 -0.05248002 -0.10100187 -0.06256259  0.03736418\n",
            " -0.05366392 -0.06844444 -0.04377154  0.05119866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fn7ZD5I7repP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "cc5d5370-ea70-44c3-bf2a-5a30f5ff1fd5"
      },
      "cell_type": "code",
      "source": [
        "# Here we use PCS to plot the relatios between \n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose Your Favourite list of words, or make a new list on your own\n",
        "Favourites = ['Men','Women','King','Queen','boy','girl']\n",
        "Favourites = ['cat','cats','dog','dogs']\n",
        "# NOTE: You MIGHT not be ablt to see the word relations in the plot. This may be due to small amount of training data and dimentionality reduction\n",
        "\n",
        "Favourites = [i.lower() for i in Favourites]\n",
        "Vectors = [vectors[w] if w in vectors else np.zeros_like(vectors['a']) for w in Favourites]\n",
        "for w,v in zip(Favourites[:2],Vectors[:2]):\n",
        "    print( \"{}\\n{}\\n\".format(w,v.tolist()) )\n",
        "    break\n",
        "    \n",
        "# Visualize these vectors on your own in 2d\n",
        "pca = PCA(n_components=2)\n",
        "vectors_2d = pca.fit_transform(np.asarray(Vectors))\n",
        "\n",
        "z, y = vectors_2d[:,0], vectors_2d[:,1]\n",
        "labels = Favourites\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(z, y)\n",
        "for i, txt in enumerate(labels):\n",
        "    ax.annotate(txt, (z[i], y[i]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n",
            "[0.011262456886470318, -0.016896279528737068, 0.011470533907413483, -0.006318163126707077, -0.0030332286842167377, -0.009637177921831608, -0.0032776412554085255, 0.021074824035167694, -0.010111209936439991, 0.013182228431105614, 0.018292222172021866, 0.005568765103816986, 0.013900133781135082, 0.012405448593199253, -0.022972526028752327, -0.01998080126941204, -0.01566762663424015, -0.00893367175012827, 0.005097092129290104, -0.003047030884772539, -0.015960508957505226, 0.0038710683584213257, 0.020978868007659912, 0.01063678227365017, -0.010502125136554241, 0.006686170119792223, -0.0017585186287760735, 0.01567201130092144, -0.01047380082309246, -0.0044497339986264706, -0.013111229985952377, 0.011917724274098873, 0.00913666095584631, -0.014782913960516453, 0.015740258619189262, -0.013865184038877487, -0.017297767102718353, -0.007566735614091158, -0.00914730317890644, -0.017680050805211067, 0.00611748406663537, 0.008978700265288353, -0.004375886172056198, -0.012217050418257713, -0.022354839369654655, -0.012490391731262207, 0.004335949197411537, 0.004920699167996645, 0.003321841126307845, -0.013201292604207993, -0.006028033792972565, -0.005389341618865728, 0.0011763202492147684, 0.011422746814787388, -0.0058033824898302555, 0.014059140346944332, 0.0069332304410636425, -0.011298591271042824, 0.0156403761357069, -0.014848432503640652, 0.014898067340254784, 0.018640311434864998, -0.02137419953942299, 0.008035512641072273, 0.013480167835950851, 0.009472062811255455, 0.011688554659485817, -0.0040823593735694885, -0.0065820589661598206, 0.0135038485750556, -0.004470895044505596, 0.01303119771182537, 0.006432885304093361, -0.004670604597777128, -0.011151330545544624, -0.008646299131214619, -0.021474450826644897, 0.003609583480283618, 0.016179440543055534, 0.007661140989512205, 0.007674904074519873, 0.004232930950820446, 0.01168340165168047, 0.012595581822097301, 0.013926118612289429, 0.012928715907037258, -0.021657971665263176, -0.014814620837569237, 0.02052287757396698, -0.012146424502134323, 0.008754285052418709, 0.006138491909950972, -0.01268056221306324, -0.011305723339319229, -0.014982917346060276, 0.018117841333150864, -0.0042555369436740875, -0.00898727122694254, -0.015462864190340042, 0.020255347713828087]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF9ZJREFUeJzt3XuQlfWd5/H311YcFOQiTIwgAutl\nCqHFob2gJrirE4xZRVwRw0wkrqPDzmCya0lCLrWDLibgZVw34w5YGoOYHXStaHAwQxkJJQmRoQkd\n2I4hoNFCRIcIzaBiCDPf/aOPpB/Spps+hz7d9vtVdYrn8j3nfH/VwOc8z3P6+UVmIknS+46odgOS\npK7FYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySp4MhqN9ARgwYNyuHDh1e7DUnq\nVtatW/erzBzcVl23DIbhw4dTX19f7TYkqVuJiFfbU+epJElSgcEgSSowGCRJBQaDJKnAYJAkFRgM\nkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySpwGCQJBUYDJKkAoNBklRQkWCIiEsjYlNEbImI\n2a3sPzoiHivtXxMRw1vsq42IH0dEY0RsjIg/qERPkqSOKTsYIqIGuB/4JDAK+HREjDqo7AZgV2ae\nAtwLzC8990jgUWBGZp4BXAT8ptyeJEkdV4kjhnOALZn5cmbuA5YAkw6qmQQsKi0/AVwcEQF8AtiQ\nmT8FyMy3MvNfK9CTJKmDKhEMQ4CtLdZfK21rtSYz9wO7geOB04CMiOUR8ZOI+EIF+pEklaHacz4f\nCVwInA28CzwXEesy87mDCyPiJuAmgGHDhnVqk5LUk1TiiGEbcFKL9aGlba3WlK4r9APeovno4vnM\n/FVmvgs8A/xxa2+SmQ9kZl1m1g0ePLgCbUuSWlOJYFgLnBoRIyKiF3AtsPSgmqXA9NLy1cCKzExg\nOTAmIo4pBcYE4GcV6EmS1EFln0rKzP0RMZPm/+RrgG9mZmNE3A7UZ+ZS4CFgcURsAXbSHB5k5q6I\n+BuawyWBZzJzWbk9SZI6Lpo/uHcvdXV1WV9fX+02JKlbKV3DrWurzt98liQVGAySpAKDQZJUYDBI\nwJw5c7j77rur3YbUJRgMkqQCg0E91h133MFpp53GhRdeyKZNmwBoaGjgvPPOo7a2lsmTJ7Nr1y4A\n1q5dS21tLWPHjmXWrFmMHj0agMbGRs455xzGjh1LbW0tmzdvrtp4pEoxGNQjrVu3jiVLltDQ0MAz\nzzzD2rVrAbjuuuuYP38+GzZsYMyYMdx2220AXH/99SxcuJCGhgZqamoOvM6CBQv4/Oc/T0NDA/X1\n9QwdOrQq45EqyWBQj7Rq1SomT57MMcccw3HHHccVV1zBO++8Q1NTExMmTABg+vTpPP/88zQ1NbFn\nzx7Gjx8PwLRp0w68zvjx4/na177G/PnzefXVV+ndu3dVxiNVksGgHuOp9du4YN4KRsxexn3f38zP\n3/iXsl9z2rRpLF26lN69e3PZZZexYsWKCnQqVZfBoB7hqfXb+NJ3NrKtaS8JvDfoNJZ+97s89uMt\n7Nmzh6effppjjz2WAQMGsGrVKgAWL17MhAkT6N+/P3379mXNmjUALFmy5MDrvvzyy4wcOZLPfe5z\nTJo0iQ0bNlRjeFJFVfu221KnuGv5Jvb+5rdzQB19win0Pv1jfPbyixj3R8M5++yzAVi0aBEzZszg\n3XffZeTIkTz88MMAPPTQQ9x4440cccQRTJgwgX79+gHw+OOPs3jxYo466ihOOOEEvvzlL3f+4KQK\n815J6hFGzF5Ga3/TA/jlvE+1+fy3336bPn36ADBv3jy2b9/OfffdV9kmpcOsvfdK8ohBPcKJ/Xuz\nrWlvq9vbY9myZXz9619n//79nHzyyXzrW9+qcIdS12EwqEeYNfF0vvSdjYXTSb2PqmHWxNPb9fyp\nU6cyderUw9We1KUYDOoRrjyreRryu5Zv4vWmvZzYvzezJp5+YLuk3zIY1GNcedYQg0BqB7+uKkkq\nMBgkSQUGgySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBgkCQVGAySpAKD\nQZJUUJFgiIhLI2JTRGyJiNmt7D86Ih4r7V8TEcMP2j8sIt6OiFsr0Y8kqePKDoaIqAHuBz4JjAI+\nHRGjDiq7AdiVmacA9wLzD9r/N8D3yu1FklS+ShwxnANsycyXM3MfsASYdFDNJGBRafkJ4OKICICI\nuBL4JdBYgV4kSWWqRDAMAba2WH+ttK3VmszcD+wGjo+IPsAXgdvaepOIuCki6iOifseOHRVoW5LU\nmmpffJ4D3JuZb7dVmJkPZGZdZtYNHjz48HcmST1UJeZ83gac1GJ9aGlbazWvRcSRQD/gLeBc4OqI\nuBPoD/xbRLyXmX9bgb4kSR1QiWBYC5waESNoDoBrgWkH1SwFpgM/Bq4GVmRmAh97vyAi5gBvGwqS\nVF1lB0Nm7o+ImcByoAb4ZmY2RsTtQH1mLgUeAhZHxBZgJ83hIUnqgqL5g3v3UldXl/X19dVuQ5K6\nlYhYl5l1bdVV++KzJKmLMRgkSQUGgySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkF\nBoMkqcBgkCQVGAySpAKDQZJUYDBIkgoMBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAw\nSJIKDAZJUoHBIEkqMBgkSQUGgySpwGCQJBUYDJKkgooEQ0RcGhGbImJLRMxuZf/REfFYaf+aiBhe\n2v4nEbEuIjaW/vwPlehHktRxZQdDRNQA9wOfBEYBn46IUQeV3QDsysxTgHuB+aXtvwIuz8wxwHRg\ncbn9SJLKU4kjhnOALZn5cmbuA5YAkw6qmQQsKi0/AVwcEZGZ6zPz9dL2RqB3RBxdgZ4kSR1UiWAY\nAmxtsf5aaVurNZm5H9gNHH9QzX8CfpKZv27tTSLipoioj4j6HTt2VKBtSVJrusTF54g4g+bTS3/x\nQTWZ+UBm1mVm3eDBgzuvOUnqYSoRDNuAk1qsDy1ta7UmIo4E+gFvldaHAk8C12XmSxXoR5JUhkoE\nw1rg1IgYERG9gGuBpQfVLKX54jLA1cCKzMyI6A8sA2Zn5o8q0IskqUxlB0PpmsFMYDnwIvB4ZjZG\nxO0RcUWp7CHg+IjYAtwCvP+V1pnAKcB/j4iG0uMPy+1JktRxkZnV7uGQ1dXVZX19fbXbkKRuJSLW\nZWZdW3Vd4uKzJKnrMBgkSQUGgySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMk\nqcBgkCQVGAySpAKDQZJUYDBIkgoMBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIK\nDAZJUoHBIEkqMBgkSQUGgySpwGCQJBUYDJKkgooEQ0RcGhGbImJLRMxuZf/REfFYaf+aiBjeYt+X\nSts3RcTESvQjSeq4soMhImqA+4FPAqOAT0fEqIPKbgB2ZeYpwL3A/NJzRwHXAmcAlwL/u/R6kqQq\nqcQRwznAlsx8OTP3AUuASQfVTAIWlZafAC6OiChtX5KZv87MXwJbSq8nSaqSSgTDEGBri/XXStta\nrcnM/cBu4Ph2PleS1Im6zcXniLgpIuojon7Hjh3VbkeSPrQqEQzbgJNarA8tbWu1JiKOBPoBb7Xz\nuQBk5gOZWZeZdYMHD65A25Kk1lQiGNYCp0bEiIjoRfPF5KUH1SwFppeWrwZWZGaWtl9b+tbSCOBU\n4J8q0JMkqYOOLPcFMnN/RMwElgM1wDczszEibgfqM3Mp8BCwOCK2ADtpDg9KdY8DPwP2A3+Vmf9a\nbk+SpI6L5g/u3UtdXV3W19dXuw1J6lYiYl1m1rVV120uPkuSOofBIEkqMBgkSQUGgySpwGCQJBUY\nDJKkAoNBklRgMByilStXsnr16mq3IUmHjcFwiAwGSR92Zd8S48PikUce4e677yYiqK2t5ZprrmHu\n3Lns27eP448/nm9/+9vs3buXBQsWUFNTw6OPPso3vvEN3njjDW677TZqamro168fzz//fLWHIkll\nMRiAxsZG5s6dy+rVqxk0aBA7d+4kInjhhReICB588EHuvPNO7rnnHmbMmEGfPn249dZbARgzZgzL\nly9nyJAhNDU1VXkkklQ+gwFYsWIFU6ZMYdCgQQAMHDiQjRs3MnXqVLZv386+ffsYMWJEq8+94IIL\n+OxnP8s111zDVVdd1ZltS9Jh0aOvMTy1fhsXzFvBnKWNLFr9Ck+t/+1UEDfffDMzZ85k48aNLFy4\nkPfee6/V11iwYAFz585l69atjBs3jrfeequz2pekw6LHBsNT67fxpe9sZFvTXo4eVsubP13JFx79\nEU+t38bOnTvZvXs3Q4Y0zzK6aNGiA8/r27cve/bsObD+0ksvce6553L77bczePBgtm7d+jvvJUnd\nSY8NhruWb2Lvb5qnfug1+GT6jZ/KK4/M4k8v+zi33HILc+bMYcqUKYwbN+7AKSaAyy+/nCeffJKx\nY8eyatUqZs2axZgxYxg9ejTnn38+Z555ZrWGJEkV0WPnYxgxexmtjTyAX877VFmvLUldkfMxtOHE\n/r0Pabsk9RQ9NhhmTTyd3kfVFLb1PqqGWRNPr1JHktQ19Nivq155VvOF5buWb+L1pr2c2L83syae\nfmC7JPVUPTYYoDkcDAJJKuqxp5IkqbuZM2cOd99992F/H4NBklRgMEhSF3bHHXdw2mmnceGFF7Jp\n0yYAGhoaOO+886itrWXy5Mns2rULgLVr11JbW8vYsWOZNWsWo0eP7tB7GgyS1EWtW7eOJUuW0NDQ\nwDPPPMPatWsBuO6665g/fz4bNmxgzJgx3HbbbQBcf/31LFy4kIaGBmpqan7fS/9eBoMkdVGrVq1i\n8uTJHHPMMRx33HFcccUVvPPOOzQ1NTFhwgQApk+fzvPPP09TUxN79uxh/PjxAEybNq3D79ujv5Uk\nSV3RU+u3cdfyTbz47M84lr388fptnfoNSo8YJKkLKdzg86QzeHPjD/niY/X8/Q838fTTT3Pssccy\nYMAAVq1aBcDixYuZMGEC/fv3p2/fvqxZswaAJUuWdLgHjxgkqQtpeYPPo084hWP/6GO8/MBf8hdL\nBvIfzz8baL7j84wZM3j33XcZOXIkDz/8MAAPPfQQN954I0cccQQTJkygX79+HerBYJCkLuT1pr2F\n9X7nT6Xf+VMJ4P+0uMHnCy+88DvPPeOMM9iwYQMA8+bNo66uzfvltaqsU0kRMTAino2IzaU/B3xA\n3fRSzeaImF7adkxELIuIn0dEY0TMK6cXSfowKOcGn8uWLWPs2LGMHj2aVatW8dWvfrVDPZR12+2I\nuBPYmZnzImI2MCAzv3hQzUCgHqgDElgHjAN+DZybmT+IiF7Ac8DXMvN7bb1vJW67LUld0fvXGN4/\nnQTNN/j8+lVjyr4A3Vm33Z4EvD+92SLgylZqJgLPZubOzNwFPAtcmpnvZuYPADJzH/ATYGiZ/UhS\nt3blWUP4+lVjGNK/NwEM6d+7IqFwKMq9xvCRzNxeWn4D+EgrNUOAlvNdvlbadkBE9AcuB+4rsx9J\n6vaqfYPPNoMhIr4PnNDKrq+0XMnMjIhDPi8VEUcCfw/8r8x8+ffU3QTcBDBs2LBDfRsdopUrV9Kr\nVy/OP//8arciqZO1GQyZeckH7YuINyPio5m5PSI+CvxzK2XbgItarA8FVrZYfwDYnJn/s40+HijV\nUldX1/3mI+1mVq5cSZ8+fQwGqQcq9xrDUmB6aXk68N1WapYDn4iIAaVvLX2itI2ImAv0A/5rmX2o\nnR555BFqa2s588wz+cxnPsPTTz/Nueeey1lnncUll1zCm2++ySuvvMKCBQu49957GTt27IFfpJHU\nM5T7raTjgceBYcCrwDWZuTMi6oAZmfnnpbr/DHy59LQ7MvPhiBhK87WHn9P8DSWAv83MB9t6X7+V\n1DGNjY1MnjyZ1atXM2jQIHbu3ElE0L9/fyKCBx98kBdffJF77rmHOXPm0KdPH2699dZqty2pQtr7\nraSyLj5n5lvAxa1srwf+vMX6N4FvHlTzGhDlvL8OzYoVK5gyZQqDBg0CYODAgWzcuJGpU6eyfft2\n9u3bx4gRI6rcpaRq815JPcBT67dxwbwVzFnayKLVr/DU+m0H9t18883MnDmTjRs3snDhQt57770q\ndiqpKzAYPuQKN+QaVsubP13JFx79EU+t38bOnTvZvXs3Q4Y0fy1u0aJFB57Xt29f9uzZU622JVWR\nwfAh1/KGXL0Gn0y/8VN55ZFZ/OllH+eWW25hzpw5TJkyhXHjxh04xQRw+eWX8+STT3rxWeqByrr4\nXC1efG6/EbOX0dpPOIBftrghl6QPv866JYa6uHJuyCWpZzIYPuRmTTyd3kcV537tfVQNsyaeXqWO\nJHV1zsfwIff+/VbuWr6J15v2cmL/3syaeHpV78MiqWszGHqAat+QS1L34qkkSVKBwSBJKjAYJEkF\nBoMkqcBgkCQVGAySpAKDQZJUYDBIkgoMBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAw\nSJIKDAZJUoHBIEkqMBgkSQUGgySpwGCQJBWUFQwRMTAino2IzaU/B3xA3fRSzeaImN7K/qUR8f/K\n6UWSVBnlHjHMBp7LzFOB50rrBRExEPhr4FzgHOCvWwZIRFwFvF1mH5KkCik3GCYBi0rLi4ArW6mZ\nCDybmTszcxfwLHApQET0AW4B5pbZhySpQsoNho9k5vbS8hvAR1qpGQJsbbH+WmkbwP8A7gHeLbMP\nSVKFHNlWQUR8HzihlV1fabmSmRkR2d43joixwL/LzP8WEcPbUX8TcBPAsGHD2vs2kqRD1GYwZOYl\nH7QvIt6MiI9m5vaI+Cjwz62UbQMuarE+FFgJjAfqIuKVUh9/GBErM/MiWpGZDwAPANTV1bU7gCRJ\nh6bcU0lLgfe/ZTQd+G4rNcuBT0TEgNJF508AyzPz7zLzxMwcDlwI/OKDQkGS1HnKDYZ5wJ9ExGbg\nktI6EVEXEQ8CZOZOmq8lrC09bi9tkyR1QZHZ/c7K1NXVZX19fbXbkKRuJSLWZWZdW3X+5rMkqaBb\nHjFExA7g1Sq3MQj4VZV7qDTH1D04pu6hK47p5Mwc3FZRtwyGriAi6ttzSNadOKbuwTF1D915TJ5K\nkiQVGAySpAKDoeMeqHYDh4Fj6h4cU/fQbcfkNQZJUoFHDJKkAoOhndozKVFEjI2IH0dEY0RsiIip\n1ei1vQ5hoqV/jIimiPiHzu6xvSLi0ojYFBFbIqK1eUGOjojHSvvXtOfGjdXWjjF9PCJ+EhH7I+Lq\navR4qNoxplsi4melfz/PRcTJ1ejzULRjTDMiYmNENETEDyNiVDX6PCSZ6aMdD+BOYHZpeTYwv5Wa\n04BTS8snAtuB/tXuvZwxlfZdDFwO/EO1e/6A/mqAl4CRQC/gp8Cog2r+ElhQWr4WeKzafVdgTMOB\nWuAR4Opq91yhMf174JjS8n/5kPycjmuxfAXwj9Xuu62HRwzt1+akRJn5i8zcXFp+nea7zbb5yyRV\n1J6JlsjM54A9ndVUB5wDbMnMlzNzH7CE5rG11HKsTwAXR0R0Yo+Hqs0xZeYrmbkB+LdqNNgB7RnT\nDzLz/flZXqD5bsxdWXvG9C8tVo8FuvyFXYOh/dozKdEBEXEOzZ8gXjrcjZXhkMbUhf2+yaB+pyYz\n9wO7geM7pbuOac+YuptDHdMNwPcOa0fla9eYIuKvIuIlmo/SP9dJvXVYm/Mx9CSVmpSoNDfFYmB6\nZlb109zhmmhJOpwi4s+AOmBCtXuphMy8H7g/IqYBX+W30xV0SQZDC1n+pERExHHAMuArmfnCYWq1\n3Soxpm5gG3BSi/WhpW2t1bwWEUcC/YC3Oqe9DmnPmLqbdo0pIi6h+YPLhMz8dSf11lGH+nNaAvzd\nYe2oAjyV1H5tTkoUEb2AJ4FHMvOJTuyto9oz0VJ3sBY4NSJGlH4G19I8tpZajvVqYEWWrgZ2Ue0Z\nU3fT5pgi4ixgIXBFZnaHDyrtGdOpLVY/BWzuxP46ptpXv7vLg+bz0c/R/EP9PjCwtL0OeLC0/GfA\nb4CGFo+x1e69nDGV1lcBO4C9NJ9DnVjt3lsZy2XAL2i+pvOV0rbbaf4PBuAPgP8LbAH+CRhZ7Z4r\nMKazSz+Pd2g++mmsds8VGNP3gTdb/PtZWu2eKzCm+4DG0nh+AJxR7Z7bevibz5KkAk8lSZIKDAZJ\nUoHBIEkqMBgkSQUGgySpwGCQJBUYDJKkAoNBklTw/wFkUONvesrlHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "B856joJAgYRR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation Time\n",
        "\n",
        "There are two ways to evaluate word vectors:\n",
        " - Intrinsic evaluation : Evaluate the vectors by retrieving similar words from the vocabulary.\n",
        " - Extrinsic Evaluation : Evaluate the vectors by using them to train a model on a downstream task.\n",
        " \n",
        " In this assignment we will do some extrinsic evaluation, here, the downstream task being Polarity classification.\n",
        " Don't worry you don't need to write any code here. Just follow the code and run the cells.\n",
        " Make sure that you go through the CNN Model -> fit() function. It is the heart of the keras CNN code for text processing."
      ]
    },
    {
      "metadata": {
        "id": "4ZAm5UNngOOH",
        "colab_type": "code",
        "outputId": "2775f1a7-c694-4bbe-b1cb-d0afe9834bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# PLEASE DO NOT MODIFY THIS CODE\n",
        "# Load Polarity Data\n",
        "data_dir = 'gdrive/My Drive/'\n",
        "import pickle\n",
        "all_data = pickle.load(open(data_dir+'polarity.pkl', 'rb'))\n",
        "\n",
        "data = [d.lower() for d in all_data['data']]\n",
        "labels = all_data['labels']\n",
        "label_names = all_data['label_names']\n",
        "val = [d.lower() for d in all_data['imdb']]\n",
        "val_labels = all_data['imdb_labels']\n",
        "print(len(data), len(val))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10433 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j6VIH96MhqGy",
        "colab_type": "code",
        "outputId": "7d5b5590-29aa-4175-9be7-8e1987ed9e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# PLEASE DO NOT MODIFY THIS CODE\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "import keras.preprocessing.text\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.datasets import imdb\n",
        "import tempfile\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def get_most_common_embeddings_glove(tokenizer, glove):\n",
        "    import operator\n",
        "    most_common = list(map(operator.itemgetter(0), sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))))\n",
        "    n = len(tokenizer.word_index)\n",
        "    if tokenizer.num_words is not None:\n",
        "        most_common = most_common[:tokenizer.num_words]\n",
        "        n = min(tokenizer.num_words, n)\n",
        "    embeddings = np.zeros((n + 1, glove['is'].shape[0]), dtype='float32')\n",
        "    for i, lex in enumerate(most_common):\n",
        "        if lex in glove :\n",
        "            embeddings[i + 1] = glove[lex]\n",
        "    return embeddings\n",
        "\n",
        "class CNNClassifier:\n",
        "    def __init__(self, glove):\n",
        "        self.glove = glove\n",
        "        pass  \n",
        "    def predict(self, X):\n",
        "        return self.predict_proba(X).argmax(axis=1)\n",
        "    def predict_proba(self, X):\n",
        "        x_test = self.tokenizer.texts_to_sequences(X)\n",
        "        x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)\n",
        "        a = self.model.predict(x_test, verbose=0).flatten()\n",
        "        a = a.reshape(-1, 1)\n",
        "        return np.hstack((1 - a, a))\n",
        "    def fit(self, X, Y, max_features=20000, maxlen=400,\n",
        "            batch_size=32, hidden_dims=250, filters=250, kernel_size=3,\n",
        "            epochs=5):\n",
        "        from keras.preprocessing import sequence\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers import Dense, Dropout, Activation\n",
        "        from keras.layers import Embedding\n",
        "        from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "        self.tokenizer = keras.preprocessing.text.Tokenizer(\n",
        "            num_words=max_features, split=\" \", char_level=False)\n",
        "        self.tokenizer.fit_on_texts(X)\n",
        "        x_train = self.tokenizer.texts_to_sequences(X)\n",
        "        self.maxlen = maxlen\n",
        "#         embeddings = get_most_common_embeddings(self.tokenizer, self.nlp)\n",
        "        embeddings = get_most_common_embeddings_glove(self.tokenizer, self.glove)\n",
        "        x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)\n",
        "        self.model = Sequential()\n",
        "        # we start off with an efficient embedding layer which maps\n",
        "        # our vocab indices into embedding_dims dimensions\n",
        "        self.model.add(\n",
        "            Embedding(\n",
        "                embeddings.shape[0],\n",
        "                embeddings.shape[1],\n",
        "                input_length=maxlen,\n",
        "                trainable=False,\n",
        "                weights=[embeddings]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.model.add(Dropout(0.2))\n",
        "\n",
        "        # we add a Convolution1D, which will learn filters\n",
        "        # word group filters of size filter_length:\n",
        "        self.model.add(Conv1D(filters, kernel_size, padding='valid',\n",
        "                              activation='relu', strides=1))\n",
        "        # we use max pooling:\n",
        "        self.model.add(GlobalMaxPooling1D())\n",
        "\n",
        "        # We add a vanilla hidden layer:\n",
        "        self.model.add(Dense(hidden_dims))\n",
        "        self.model.add(Dropout(0.2))\n",
        "        self.model.add(Activation('relu'))\n",
        "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "        self.model.add(Dense(1))\n",
        "        # model.add(Dense(3))\n",
        "        self.model.add(Activation('sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "        # optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "        optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "        # model.compile(loss='categorical_crossentropy',\n",
        "        #               optimizer=optimizer,\n",
        "        #               metrics=['accuracy'])\n",
        "        self.model.compile(loss='binary_crossentropy',\n",
        "                           optimizer=optimizer,\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "        self.model.fit(x_train, Y, batch_size=batch_size, epochs=epochs, verbose=2)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5A96rcw_hxox",
        "colab_type": "code",
        "outputId": "ea84eb74-7e3b-464c-e487-9ce7f28f5fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1941
        }
      },
      "cell_type": "code",
      "source": [
        "cnnmodel = CNNClassifier(vectors)\n",
        "cnnmodel.fit(data, labels, epochs=50, maxlen=100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.6824 - acc: 0.5656\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.6730 - acc: 0.5856\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.6677 - acc: 0.5909\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.6664 - acc: 0.5918\n",
            "Epoch 5/50\n",
            " - 10s - loss: 0.6611 - acc: 0.6072\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.6588 - acc: 0.6020\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.6557 - acc: 0.6124\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.6540 - acc: 0.6140\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.6497 - acc: 0.6173\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.6459 - acc: 0.6247\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.6445 - acc: 0.6240\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.6408 - acc: 0.6294\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.6401 - acc: 0.6270\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.6375 - acc: 0.6349\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.6363 - acc: 0.6366\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.6347 - acc: 0.6382\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.6331 - acc: 0.6386\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.6307 - acc: 0.6409\n",
            "Epoch 19/50\n",
            " - 11s - loss: 0.6298 - acc: 0.6401\n",
            "Epoch 20/50\n",
            " - 11s - loss: 0.6280 - acc: 0.6438\n",
            "Epoch 21/50\n",
            " - 11s - loss: 0.6244 - acc: 0.6523\n",
            "Epoch 22/50\n",
            " - 11s - loss: 0.6228 - acc: 0.6507\n",
            "Epoch 23/50\n",
            " - 11s - loss: 0.6206 - acc: 0.6535\n",
            "Epoch 24/50\n",
            " - 11s - loss: 0.6188 - acc: 0.6535\n",
            "Epoch 25/50\n",
            " - 11s - loss: 0.6199 - acc: 0.6579\n",
            "Epoch 26/50\n",
            " - 11s - loss: 0.6151 - acc: 0.6606\n",
            "Epoch 27/50\n",
            " - 11s - loss: 0.6119 - acc: 0.6611\n",
            "Epoch 28/50\n",
            " - 11s - loss: 0.6117 - acc: 0.6537\n",
            "Epoch 29/50\n",
            " - 11s - loss: 0.6123 - acc: 0.6557\n",
            "Epoch 30/50\n",
            " - 11s - loss: 0.6099 - acc: 0.6598\n",
            "Epoch 31/50\n",
            " - 11s - loss: 0.6094 - acc: 0.6639\n",
            "Epoch 32/50\n",
            " - 11s - loss: 0.6065 - acc: 0.6676\n",
            "Epoch 33/50\n",
            " - 11s - loss: 0.6040 - acc: 0.6703\n",
            "Epoch 34/50\n",
            " - 11s - loss: 0.6034 - acc: 0.6627\n",
            "Epoch 35/50\n",
            " - 11s - loss: 0.6030 - acc: 0.6674\n",
            "Epoch 36/50\n",
            " - 11s - loss: 0.6010 - acc: 0.6762\n",
            "Epoch 37/50\n",
            " - 11s - loss: 0.6017 - acc: 0.6686\n",
            "Epoch 38/50\n",
            " - 11s - loss: 0.5964 - acc: 0.6794\n",
            "Epoch 39/50\n",
            " - 11s - loss: 0.5969 - acc: 0.6723\n",
            "Epoch 40/50\n",
            " - 11s - loss: 0.5975 - acc: 0.6752\n",
            "Epoch 41/50\n",
            " - 11s - loss: 0.5977 - acc: 0.6732\n",
            "Epoch 42/50\n",
            " - 11s - loss: 0.5961 - acc: 0.6781\n",
            "Epoch 43/50\n",
            " - 11s - loss: 0.5951 - acc: 0.6796\n",
            "Epoch 44/50\n",
            " - 11s - loss: 0.5919 - acc: 0.6778\n",
            "Epoch 45/50\n",
            " - 11s - loss: 0.5891 - acc: 0.6799\n",
            "Epoch 46/50\n",
            " - 11s - loss: 0.5908 - acc: 0.6806\n",
            "Epoch 47/50\n",
            " - 11s - loss: 0.5882 - acc: 0.6860\n",
            "Epoch 48/50\n",
            " - 11s - loss: 0.5833 - acc: 0.6817\n",
            "Epoch 49/50\n",
            " - 11s - loss: 0.5880 - acc: 0.6853\n",
            "Epoch 50/50\n",
            " - 11s - loss: 0.5839 - acc: 0.6861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXmYi_gxh7So",
        "colab_type": "code",
        "outputId": "b335611e-ef06-446f-95bd-267fca74f818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "val_accuracy = (cnnmodel.predict(val) == val_labels).mean()\n",
        "print(val_accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MqVZryF0p4Kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('out.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUwNu0iWte9x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dx5Jpr1uZUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}